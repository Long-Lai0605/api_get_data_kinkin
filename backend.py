import gspread
import requests
import pandas as pd
import math
import time
import toml
import json
from datetime import datetime, timedelta
from google.oauth2.service_account import Credentials
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlencode, quote

# --- C·∫§U H√åNH ---
SCOPE = ["https://www.googleapis.com/auth/spreadsheets", "https://www.googleapis.com/auth/drive"]

def get_connection(secrets_dict):
    try:
        if not secrets_dict: return None, "Secrets is empty"
        creds = Credentials.from_service_account_info(secrets_dict["gcp_service_account"], scopes=SCOPE)
        gc = gspread.authorize(creds)
        master_id = secrets_dict["system"]["master_sheet_id"]
        return gc.open_by_key(master_id), "Success"
    except Exception as e: return None, str(e)

def init_database(secrets_dict):
    sh, msg = get_connection(secrets_dict)
    if not sh: return
    
    schemas = {
        "luu_cau_hinh": ["Block Name", "Tr·∫°ng th√°i", "Ng√†y b·∫Øt ƒë·∫ßu", "Ng√†y k·∫øt th√∫c", "Filter Key", "Link ƒê√≠ch", "Sheet ƒê√≠ch", "Last Run", "Total Rows"],
        "log_api_1office": ["Block Name", "Method", "API URL", "Access Token (Encrypted)"],
        "lich_chay_tu_dong": ["Lo·∫°i l·ªãch", "Chi ti·∫øt", "C·∫≠p nh·∫≠t l√∫c"] # B·∫£ng m·ªõi l∆∞u c·∫•u h√¨nh h·∫πn gi·ªù
    }
    
    existing = [s.title for s in sh.worksheets()]
    for name, cols in schemas.items():
        if name not in existing:
            try:
                wks = sh.add_worksheet(name, 100, 20)
                wks.append_row(cols)
            except: pass

# --- [M·ªöI] H√ÄM L∆ØU C·∫§U H√åNH T·ª™ DASHBOARD ---
def save_configurations(secrets_dict, df_editor):
    """L∆∞u d·ªØ li·ªáu t·ª´ st.data_editor xu·ªëng sheet luu_cau_hinh"""
    try:
        sh, _ = get_connection(secrets_dict)
        wks = sh.worksheet("luu_cau_hinh")
        
        # L·∫•y d·ªØ li·ªáu c≈© ƒë·ªÉ gi·ªØ l·∫°i c√°c c·ªôt kh√¥ng hi·ªÉn th·ªã tr√™n dashboard (n·∫øu c√≥)
        # ·ªû ƒë√¢y ta gi·∫£ ƒë·ªãnh df_editor ƒë√£ ch·ª©a ƒë·ªß c√°c c·ªôt c·∫ßn thi·∫øt ƒë·ªÉ overwrite
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ ghi
        # df_editor l√† DataFrame ƒë√£ edit
        # C·∫ßn ƒë·∫£m b·∫£o th·ª© t·ª± c·ªôt kh·ªõp v·ªõi Schema:
        # ["Block Name", "Tr·∫°ng th√°i", "Ng√†y b·∫Øt ƒë·∫ßu", "Ng√†y k·∫øt th√∫c", "Filter Key", "Link ƒê√≠ch", "Sheet ƒê√≠ch", "Last Run", "Total Rows"]
        
        required_cols = ["Block Name", "Tr·∫°ng th√°i", "Ng√†y b·∫Øt ƒë·∫ßu", "Ng√†y k·∫øt th√∫c", "Filter Key", "Link ƒê√≠ch", "Sheet ƒê√≠ch", "Last Run", "Total Rows"]
        
        # ƒê·∫£m b·∫£o ƒë·ªß c·ªôt (n·∫øu thi·∫øu th√¨ fill r·ªóng)
        for col in required_cols:
            if col not in df_editor.columns:
                df_editor[col] = ""
                
        # S·∫Øp x·∫øp ƒë√∫ng th·ª© t·ª±
        df_to_save = df_editor[required_cols]
        
        # Chuy·ªÉn v·ªÅ list of lists
        # L∆∞u √Ω: Convert c√°c ki·ªÉu ng√†y th√°ng/s·ªë v·ªÅ string ƒë·ªÉ tr√°nh l·ªói JSON
        data_values = df_to_save.astype(str).values.tolist()
        
        # X√≥a d·ªØ li·ªáu c≈© (tr·ª´ header d√≤ng 1)
        wks.clear()
        wks.append_row(required_cols)
        wks.append_rows(data_values)
        
        return True, "ƒê√£ l∆∞u c·∫•u h√¨nh th√†nh c√¥ng!"
    except Exception as e:
        return False, f"L·ªói l∆∞u: {str(e)}"

# --- [M·ªöI] H√ÄM L∆ØU L·ªäCH CH·∫†Y ---
def save_schedule_settings(secrets_dict, schedule_type, details_json):
    try:
        sh, _ = get_connection(secrets_dict)
        try: wks = sh.worksheet("lich_chay_tu_dong")
        except: wks = sh.add_worksheet("lich_chay_tu_dong", 100, 5)
        
        wks.clear()
        wks.append_row(["Lo·∫°i l·ªãch", "Chi ti·∫øt", "C·∫≠p nh·∫≠t l√∫c"])
        
        now = datetime.now().strftime("%H:%M %d/%m/%Y")
        wks.append_row([schedule_type, json.dumps(details_json, ensure_ascii=False), now])
        return True
    except Exception as e: return False

# --- C√ÅC H√ÄM FETCH & X·ª¨ L√ù (GI·ªÆ NGUY√äN T·ª™ PHI√äN B·∫¢N TR∆Ø·ªöC) ---
def parse_date_val(date_str):
    if not date_str: return None
    s = str(date_str).strip()
    formats = ["%d/%m/%Y %H:%M:%S", "%d/%m/%Y", "%Y-%m-%d %H:%M:%S", "%Y-%m-%d", "%d-%m-%Y"]
    for fmt in formats:
        try: return datetime.strptime(s, fmt)
        except: continue
    try: return datetime.strptime(s.split(' ')[0], "%d/%m/%Y")
    except: pass
    return None

def filter_chunk_client_side(items, filter_key, date_start, date_end):
    if not filter_key or (not date_start and not date_end): return items
    filtered = []
    d_start = datetime.combine(date_start, datetime.min.time()) if date_start else None
    d_end = datetime.combine(date_end, datetime.max.time()) if date_end else None
    for item in items:
        val_str = item.get(filter_key)
        if not val_str: continue 
        val_date = parse_date_val(val_str)
        if not val_date: 
            filtered.append(item)
            continue
        if d_start and val_date < d_start: continue
        if d_end and val_date > d_end: continue
        filtered.append(item)
    return filtered

def build_manual_url(base_url, access_token, limit, page, filters_list=None):
    params = {"access_token": access_token.strip(), "limit": limit, "page": page, "sort_by": "id", "sort_type": "desc"}
    query_string = urlencode(params)
    filter_part = ""
    if filters_list:
        json_str = json.dumps(filters_list, separators=(',', ':'))
        filter_part = f"&filters={quote(json_str)}"
    return f"{base_url}?{query_string}{filter_part}"

def fetch_single_page_manual(full_url, method):
    try:
        if method.upper() == "POST": r = requests.post(full_url, json={}, timeout=30)
        else: r = requests.get(full_url, timeout=30)
        if r.status_code == 200:
            d = r.json()
            return d.get("data", d.get("items", []))
    except: pass
    return []

def fetch_1office_data_smart(url, token, method="GET", filter_key=None, date_start=None, date_end=None, status_callback=None):
    all_data = []
    limit = 50
    filters_list = None
    if filter_key and (date_start or date_end):
        f_obj = {}
        if date_start: f_obj[f"{filter_key}_from"] = date_start.strftime("%d/%m/%Y")
        if date_end: f_obj[f"{filter_key}_to"] = (date_end + timedelta(days=1)).strftime("%d/%m/%Y")
        filters_list = [f_obj]

    if status_callback: status_callback("üì° G·ªçi Page 1...")
    page1_url = build_manual_url(url, token, limit, 1, filters_list)
    
    try:
        if method.upper() == "POST": res = requests.post(page1_url, json={}, timeout=30)
        else: res = requests.get(page1_url, timeout=30)
        
        if res.status_code != 200: return None, f"HTTP {res.status_code}"
        d = res.json()
        if d.get("code") == "token_not_valid": return None, "H·∫øt h·∫°n API"
        
        total_items = d.get("total_item", 0)
        items = d.get("data", d.get("items", []))
        if items: all_data.extend(items)
        if total_items == 0 and not items: return [], "Success (0 KQ)"

        estimated_pages = math.ceil(total_items / limit)
        if estimated_pages > 1:
            if status_callback: status_callback(f"üöÄ T·∫£i {estimated_pages} trang...")
            with ThreadPoolExecutor(max_workers=10) as executor:
                futures = {executor.submit(fetch_single_page_manual, build_manual_url(url, token, limit, p, filters_list), method): p for p in range(2, estimated_pages + 1)}
                for future in as_completed(futures):
                    p_items = future.result()
                    if p_items: all_data.extend(p_items)

        # V√©t c·∫°n
        current_page = estimated_pages + 1
        max_safety = 20
        while max_safety > 0:
            extra = fetch_single_page_manual(build_manual_url(url, token, limit, current_page, filters_list), method)
            if extra:
                all_data.extend(extra)
                current_page += 1
                max_safety -= 1
            else: break
            
        return all_data, "Success"
    except Exception as e: return None, str(e)

def write_to_sheet_range(secrets_dict, block_conf, data):
    if not data: return "0", "No Data"
    try:
        creds = Credentials.from_service_account_info(secrets_dict["gcp_service_account"], scopes=SCOPE)
        gc = gspread.authorize(creds)
        dest_ss = gc.open_by_url(block_conf['Link ƒê√≠ch'])
        wks_name = block_conf['Sheet ƒê√≠ch']
        try: wks = dest_ss.worksheet(wks_name)
        except: wks = dest_ss.add_worksheet(wks_name, 1000, 20)
        wks.clear()
        
        rows = [list(data[0].keys()) + ["Link Ngu·ªìn", "Sheet Ngu·ªìn", "Th√°ng Ch·ªët", "Lu·ªìng (Block)"]]
        month = datetime.now().strftime("%m/%Y")
        for item in data:
            r = list(item.values())
            r = [str(x) if isinstance(x, (dict, list)) else x for x in r]
            r.extend([block_conf['Link ƒê√≠ch'], wks_name, month, block_conf['Block Name']])
            rows.append(r)
        wks.update(values=rows, range_name='A1')
        
        range_str = f"D√≤ng 2 -> {len(rows)}"
        update_master_status(secrets_dict, block_conf['Block Name'], range_str)
        return range_str, "Success"
    except Exception as e: return "0", str(e)

def update_master_status(secrets_dict, block_name, range_str):
    try:
        sh, _ = get_connection(secrets_dict)
        wks = sh.worksheet("luu_cau_hinh")
        cell = wks.find(block_name)
        if cell:
            vn_time = (datetime.utcnow() + timedelta(hours=7)).strftime("%H:%M %d/%m")
            wks.update_cell(cell.row, 8, vn_time)
            wks.update_cell(cell.row, 9, range_str)
    except: pass

def get_active_blocks(secrets_dict):
    sh, _ = get_connection(secrets_dict)
    if not sh: return []
    try:
        c = pd.DataFrame(sh.worksheet("luu_cau_hinh").get_all_records())
        s = pd.DataFrame(sh.worksheet("log_api_1office").get_all_records())
        if c.empty or s.empty: return []
        c.columns = [x.strip() for x in c.columns]
        s.columns = [x.strip() for x in s.columns]
        full = pd.merge(c, s, on="Block Name", how="left")
        return full.fillna("").to_dict('records')
    except: return []

def add_new_block(secrets_dict, name, method, url, token, link, sheet, start, end, filter_key):
    sh, _ = get_connection(secrets_dict)
    sh.worksheet("luu_cau_hinh").append_row([name, "Ch∆∞a ch·ªët & ƒëang c·∫≠p nh·∫≠t", str(start), str(end), filter_key, link, sheet, "", ""])
    sh.worksheet("log_api_1office").append_row([name, method, url, token.strip()])
    return True
